{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d51515",
   "metadata": {},
   "source": [
    "## Reading Web Server logs using Spark Structured Streaming\n",
    "\n",
    "As we are ready with the ability to simulate log message generation, let us get into reading these logs using Spark Structured Streaming.\n",
    "* `spark` which is of type `sparkSession` have an attribute called as `readStream`. It is of type `pyspark.sql.streaming.DataStreamReader`.\n",
    "* It exposes APIs such as `csv`, `json`, etc along with `format`. To read data from web servers, we can use `socket` as format.\n",
    "* We need to set options `host` and `port`, then invoke `load` to read data in streaming fashion.\n",
    "* It will create an object which will be of type `pyspark.sql.dataframe.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Overview of Structured Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b36ad",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50027160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "socketDF = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd78258",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b81896",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF.show() # throws exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d203960",
   "metadata": {},
   "source": [
    "* Run the below code and watch the output. You will see messages being processed every 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "\n",
    "# Triggers every 5 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
